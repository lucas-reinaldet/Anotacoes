# -*- coding: utf-8 -*-
"""Spark-Python.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wW61_2-A3gHYE6N4riHbH32IvPij-jfK

#Instalando Spark
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip
!unzip ngrok-stable-linux-amd64.zip
!pip install -q findspark

"""###Usando"""

import os
import findspark
from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf

os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

findspark.init("spark-2.4.4-bin-hadoop2.7")
conf = SparkConf().set('spark.ui.port','4050')
sc.stop() # Caso exista algum executando
sc = SparkContext(conf=conf,appName="ptiApp")
ss = SparkSession.builder.master('local[*]').getOrCreate()

#ngrok
get_ipython().system_raw('./ngrok http 4050 &')
!curl -s http://localhost:4040/api/tunnels

lista = list(range(1, 1000))
rdd = sc.parallelize(lista)
rdd.collect()

rdd = sc.textFile('/content/drive/MyDrive/AulaSparkPTI/Dados/Basilio.txt')
rdd.collect()

# Quebra em linhas
rdd = sc.textFile('/content/drive/MyDrive/AulaSparkPTI/Dados/Basilio.txt')
rdd2 = rdd.map(lambda l: len(l))
rdd2.collect()

rdd2.take(10)

type(rdd)

rdd.map(str.split).take(5)

rdd.flatMap(str.split)\
.map(lambda l: len(l))\
.take(5)

import re
rdd = sc.textFile('/content/drive/MyDrive/AulaSparkPTI/Dados/Basilio.txt')

rdd.flatMap(lambda l: re.split('\W-',l)).take(5)

rdd.flatMap(lambda l: re.split('\W+',l)).take(5)

rdd.flatMap(lambda l: re.split('\W',l)).take(5)

rdd.flatMap(lambda l: re.split('\W+',l)).map(lambda p: (p, 1)).take(5)

rdd.flatMap(lambda l: re.split('\W+',l)).map(lambda p: (p,1)).\
reduceByKey(lambda a,b: a+b).take(5)

rdd.flatMap(lambda l: re.split('\W+',l)).map(lambda p: (p,1)).\
reduceByKey(lambda a,b: a+b).\
sortByKey(False).take(5)
# True = Ordenação Crescente, False = Ordenação Decrecente

rdd.flatMap(lambda l: re.split('\W+',l)).map(lambda p: (p,1)).\
reduceByKey(lambda a,b: a+b).\
sortBy(lambda x: x[1], False).take(5)
# True = Ordenação Crescente, False = Ordenação Decrecente

rdd.flatMap(lambda l: re.split('\W+',l))\
.filter(lambda p: len(p) > 3)\
.map(lambda p: (p,1))\
.reduceByKey(lambda a,b: a+b)\
.sortBy(lambda x: x[1],False)\
.take(5)

file1 = "/content/drive/MyDrive/AulaSparkPTI/Dados/movies.txt"
file2 = "/content/drive/MyDrive/AulaSparkPTI/Dados/movies2.txt"

rdd1 = sc.textFile(file1)
rdd2 = sc.textFile(file2)
rddu = rdd1.union(rdd2)
rddu.collect()

rddu.union(rdd2).distinct().sortBy(lambda x: x[0]).collect()

rdd1.intersection(rdd2).collect()

rdd2.subtract(rdd1).collect()

rddu.count()

rddint = sc.parallelize([1,2,2,3,3,3,4,4,4,5,5,5,6,6,8,8,7,7,9,9,9,9,9,9,9,])
rddint.countByValue()

rddint.max()

rddint.min()

rddu.first()

rdd2.takeSample(False, 5)

# Quando verdadeiro, ele permite a repetição de dados
rdd2.takeSample(True, 100)

rdd2.takeSample(True, 2, seed=1) # seed é para manter sempre o mesmo valor

rddu.takeOrdered(5)

rddu.saveAsTextFile('/content/drive/MyDrive/Pós/rddu') # rddu é o nome da pasta

codec = "org.apache.hadoop.io.compress.GzipCodec"
rddu.saveAsTextFile('/content/drive/MyDrive/Pós/rddu2', codec)